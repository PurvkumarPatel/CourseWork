{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMwOv3aMHaWnYqBAKrDYtXn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install translate emoji"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1I2nbov8TYk8","executionInfo":{"status":"ok","timestamp":1706940076112,"user_tz":-330,"elapsed":6516,"user":{"displayName":"UI21CS43_ Purv Patel","userId":"12796545009456059402"}},"outputId":"42530393-b55a-4211-a131-a8deeb6f38ff"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: translate in /usr/local/lib/python3.10/dist-packages (3.6.1)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.10.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from translate) (8.1.7)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from translate) (4.9.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from translate) (2.31.0)\n","Requirement already satisfied: libretranslatepy==2.1.1 in /usr/local/lib/python3.10/dist-packages (from translate) (2.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2023.11.17)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n"],"metadata":{"id":"2KKCFWiVTu_k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706940229601,"user_tz":-330,"elapsed":6,"user":{"displayName":"UI21CS43_ Purv Patel","userId":"12796545009456059402"}},"outputId":"c2ae8e8d-4813-442e-9ba7-da4627414ca8"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["#given dataset\n","data = pd.read_csv('spam.csv', encoding='latin-1')\n","\n","train, test = train_test_split(data, test_size=0.2)\n","\n","vectorizer = TfidfVectorizer()\n","\n","X_train = vectorizer.fit_transform(train['v2'])\n","y_train = train['v1']\n","\n","X_test = vectorizer.transform(test['v2'])\n","y_test = test['v1']\n","\n","model = LogisticRegression()\n","\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_test)\n","\n","accuracy = metrics.accuracy_score(y_test, y_pred)\n","print(\"Accuracy on given dataset:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QI_x9Je8C1OC","executionInfo":{"status":"ok","timestamp":1706940235294,"user_tz":-330,"elapsed":654,"user":{"displayName":"UI21CS43_ Purv Patel","userId":"12796545009456059402"}},"outputId":"9af66085-0c87-4739-fb16-a1d830f6826e"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on given dataset: 0.9739910313901345\n"]}]},{"cell_type":"code","source":["#puncutation\n","data = pd.read_csv('spam.csv', encoding='latin-1')\n","\n","data['v2_lower'] = data['v2'].str.lower()\n","data['v2_Tokens'] = data['v2_lower'].apply(nltk.word_tokenize)\n","data['v2_No_Punctuation'] = data['v2_Tokens'].apply(lambda x: ' '.join([word for word in x if word.isalnum()]))\n","\n","stop_words = set(stopwords.words('english'))\n","data['v2_No_Stopwords'] = data['v2_No_Punctuation'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n","\n","train, test = train_test_split(data, test_size=0.2)\n","\n","if len(train['v1'].unique()) < 2:\n","    print(\"Training data contains only one class. Model training cannot proceed.\")\n","else:\n","\n","    vectorizer = TfidfVectorizer()\n","\n","    Xnp_train = vectorizer.fit_transform(train['v2_No_Punctuation'])\n","    ynp_train = train['v1']\n","\n","    Xnp_test = vectorizer.transform(test['v2_No_Punctuation'])\n","    ynp_test = test['v1']\n","    model = LogisticRegression()\n","\n","    model.fit(Xnp_train, ynp_train)\n","\n","    ynp_pred = model.predict(Xnp_test)\n","    accuracynp = metrics.accuracy_score(ynp_test, ynp_pred)\n","    print(\"Accuracy on no punctuation dataset:\", accuracynp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuqQzKB0SsXc","executionInfo":{"status":"ok","timestamp":1706940240547,"user_tz":-330,"elapsed":555,"user":{"displayName":"UI21CS43_ Purv Patel","userId":"12796545009456059402"}},"outputId":"5ad1cc83-4c79-41f4-98b7-9eb53b2622d4"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on no punctuation dataset: 0.9704035874439462\n"]}]},{"cell_type":"code","source":["#stemmed dataset\n","stemmer = PorterStemmer()\n","data['v2_Stemmed'] = data['v2_No_Stopwords'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n","\n","train, test = train_test_split(data, test_size=0.2)\n","\n","if len(train['v1'].unique()) < 2:\n","    print(\"Training data contains only one class. Model training cannot proceed.\")\n","else:\n","    vectorizer = TfidfVectorizer()\n","    Xs_train = vectorizer.fit_transform(train['v2_Stemmed'])\n","    ys_train = train['v1']\n","\n","    Xs_test = vectorizer.transform(test['v2_Stemmed'])\n","    ys_test = test['v1']\n","\n","    model = LogisticRegression()\n","\n","    model.fit(Xs_train, ys_train)\n","\n","    ys_pred = model.predict(Xs_test)\n","\n","    accuracys = metrics.accuracy_score(ys_test, ys_pred)\n","    print(\"Accuracy on stemmed dataset:\", accuracys)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gDyJ3FF0gJv2","executionInfo":{"status":"ok","timestamp":1706940245774,"user_tz":-330,"elapsed":1368,"user":{"displayName":"UI21CS43_ Purv Patel","userId":"12796545009456059402"}},"outputId":"11064fc3-e98e-4908-ec6c-f07e86a03f2a"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on stemmed dataset: 0.9587443946188341\n"]}]},{"cell_type":"code","source":["#lemetaized dataset\n","lemmatizer = WordNetLemmatizer()\n","data['v2_Lemmatized'] = data['v2_No_Stopwords'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n","\n","train, test = train_test_split(data, test_size=0.2)\n","\n","if len(train['v1'].unique()) < 2:\n","    print(\"Training data contains only one class. Model training cannot proceed.\")\n","else:\n","    vectorizer = TfidfVectorizer()\n","    Xl_train = vectorizer.fit_transform(train['v2_Lemmatized'])\n","    yl_train = train['v1']\n","\n","    Xl_test = vectorizer.transform(test['v2_Lemmatized'])\n","    yl_test = test['v1']\n","\n","    model = LogisticRegression()\n","\n","    model.fit(Xl_train, yl_train)\n","\n","    yl_pred = model.predict(Xl_test)\n","\n","    accuracyl = metrics.accuracy_score(yl_test, yl_pred)\n","    print(\"Accuracy on lemmatized dataset:\", accuracyl)\n"],"metadata":{"id":"6pjTB3ZiW9W4","executionInfo":{"status":"ok","timestamp":1706940250674,"user_tz":-330,"elapsed":39,"user":{"displayName":"UI21CS43_ Purv Patel","userId":"12796545009456059402"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9a64050-e490-4fc6-a12d-b6d6d6f137ea"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on lemmatized dataset: 0.9542600896860987\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","import nltk\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","data = pd.read_csv('spam.csv', encoding='latin-1')\n","\n","train, test = train_test_split(data, test_size=0.2, random_state=42)\n","\n","# Function to preprocess text data\n","def preprocess_data(df, column, preprocessing_function):\n","    df['processed_text'] = df[column].apply(preprocessing_function)\n","    return df\n","\n","def train_logistic_regression(X_train, y_train, X_test, y_test):\n","    vectorizer = TfidfVectorizer()\n","    X_train_tfidf = vectorizer.fit_transform(X_train)\n","    X_test_tfidf = vectorizer.transform(X_test)\n","\n","    model = LogisticRegression()\n","    model.fit(X_train_tfidf, y_train)\n","\n","    y_pred = model.predict(X_test_tfidf)\n","    accuracy = metrics.accuracy_score(y_test, y_pred)\n","    return accuracy\n","\n","# Raw Data\n","accuracy_raw = train_logistic_regression(train['v2'], train['v1'], test['v2'], test['v1'])\n","print(\"Accuracy on Raw Data:\", accuracy_raw)\n","\n","# Lowercase Data\n","train = preprocess_data(train, 'v2', lambda x: x.lower())\n","test = preprocess_data(test, 'v2', lambda x: x.lower())\n","accuracy_lowercase = train_logistic_regression(train['processed_text'], train['v1'], test['processed_text'], test['v1'])\n","print(\"Accuracy on Lowercase Data:\", accuracy_lowercase)\n","\n","# No Punctuation Data\n","train = preprocess_data(train, 'v2', lambda x: ' '.join([word for word in nltk.word_tokenize(x) if word.isalnum()]))\n","test = preprocess_data(test, 'v2', lambda x: ' '.join([word for word in nltk.word_tokenize(x) if word.isalnum()]))\n","accuracy_no_punctuation = train_logistic_regression(train['processed_text'], train['v1'], test['processed_text'], test['v1'])\n","print(\"Accuracy on No Punctuation Data:\", accuracy_no_punctuation)\n","\n","# Stemmed Data\n","stemmer = PorterStemmer()\n","train = preprocess_data(train, 'v2', lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n","test = preprocess_data(test, 'v2', lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n","accuracy_stemmed = train_logistic_regression(train['processed_text'], train['v1'], test['processed_text'], test['v1'])\n","print(\"Accuracy on Stemmed Data:\", accuracy_stemmed)\n","\n","# Lemmatized Data\n","lemmatizer = WordNetLemmatizer()\n","train = preprocess_data(train, 'v2', lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n","test = preprocess_data(test, 'v2', lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n","accuracy_lemmatized = train_logistic_regression(train['processed_text'], train['v1'], test['processed_text'], test['v1'])\n","print(\"Accuracy on Lemmatized Data:\", accuracy_lemmatized)\n","\n","# Conclusion\n","accuracies = {\n","    \"Raw Data\": accuracy_raw,\n","    \"Lowercase Data\": accuracy_lowercase,\n","    \"No Punctuation Data\": accuracy_no_punctuation,\n","    \"Stemmed Data\": accuracy_stemmed,\n","    \"Lemmatized Data\": accuracy_lemmatized\n","}\n","\n","best_preprocessing = max(accuracies, key=accuracies.get)\n","print(f\"\\nLogistic Regression provides the best accuracy with {best_preprocessing} preprocessing.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jPUZY8L4lPoS","executionInfo":{"status":"ok","timestamp":1706940506588,"user_tz":-330,"elapsed":6271,"user":{"displayName":"UI21CS43_ Purv Patel","userId":"12796545009456059402"}},"outputId":"38cab896-fd52-4c06-d3e8-f9674e5dd0bc"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy on Raw Data: 0.9659192825112107\n","Accuracy on Lowercase Data: 0.9659192825112107\n","Accuracy on No Punctuation Data: 0.9632286995515695\n","Accuracy on Stemmed Data: 0.9650224215246637\n","Accuracy on Lemmatized Data: 0.967713004484305\n","\n","Logistic Regression provides the best accuracy with Lemmatized Data preprocessing.\n"]}]}]}